{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCADA EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0.0\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "print(pa.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the base directory for raw data\n",
    "base_path = r\"C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "file_paths = {\n",
    "    \"2021 Control Threshold\": os.path.join(base_path, \"2021 Control Threshold.csv\"),\n",
    "    \"2021 Load Control\": os.path.join(base_path, \"2021 Load Control.csv\"),\n",
    "    \"2021 Online Load\": os.path.join(base_path, \"2021 Online Load.csv\"),\n",
    "    \"2022 Control Threshold\": os.path.join(base_path, \"2022 Control Threshold.csv\"),\n",
    "    \"2022 Load Control\": os.path.join(base_path, \"2022 Load Control.csv\"),\n",
    "    \"2022 Online Load\": os.path.join(base_path, \"2022 Online Load.csv\"),\n",
    "    \"2023 Control Threshold\": os.path.join(base_path, \"2023 Control Threshold.csv\"),\n",
    "    \"2023 Load Control\": os.path.join(base_path, \"2023 Load Control.csv\"),\n",
    "    \"2023 Online Load\": os.path.join(base_path, \"2023 Online Load.csv\"),\n",
    "    \"2024 Control Threshold\": os.path.join(base_path, \"2024 Control Threshold.csv\"),\n",
    "    \"2024 Load Control\": os.path.join(base_path, \"2024 Load Control.csv\"),\n",
    "    \"2024 Online Load\": os.path.join(base_path, \"2024 Online Load.csv\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All files found. Proceeding with loading...\n"
     ]
    }
   ],
   "source": [
    "# Verify that all files exist before loading\n",
    "missing_files = [name for name, path in file_paths.items() if not os.path.exists(path)]\n",
    "if missing_files:\n",
    "    print(f\"‚ö†Ô∏è Warning: The following files are missing: {missing_files}\")\n",
    "else:\n",
    "    print(\"‚úÖ All files found. Proceeding with loading...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2021 merged and saved as C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2021.csv\n",
      "‚úÖ 2022 merged and saved as C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2022.csv\n",
      "‚úÖ 2023 merged and saved as C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2023.csv\n",
      "‚úÖ 2024 merged and saved as C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to process each year's data\n",
    "def process_year(year):\n",
    "    files = {\n",
    "        \"Control Threshold\": f\"{year} Control Threshold.csv\",\n",
    "        \"Load Control\": f\"{year} Load Control.csv\",\n",
    "        \"Online Load\": f\"{year} Online Load.csv\",\n",
    "    }\n",
    "    \n",
    "    # Load datasets\n",
    "    dfs = {}\n",
    "    for key, file in files.items():\n",
    "        path = os.path.join(base_path, file)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path, parse_dates=[\"Timestamp\"])\n",
    "            df.rename(columns={df.columns[1]: key}, inplace=True)\n",
    "            dfs[key] = df\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Missing: {path}\")\n",
    "\n",
    "    # Merge three datasets for the year\n",
    "    merged_year = dfs[\"Control Threshold\"]\n",
    "    merged_year = merged_year.merge(dfs[\"Load Control\"], on=\"Timestamp\", how=\"outer\")\n",
    "    merged_year = merged_year.merge(dfs[\"Online Load\"], on=\"Timestamp\", how=\"outer\")\n",
    "\n",
    "    # Save interim file\n",
    "    output_file = os.path.join(base_path, f\"Merged_{year}.csv\")\n",
    "    merged_year.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ {year} merged and saved as {output_file}\")\n",
    "\n",
    "# Process all years\n",
    "for y in [2021, 2022, 2023, 2024]:\n",
    "    process_year(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not remove C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Final_Merged_Data.csv. It may be open in another program. Writing to temporary file instead.\n",
      "üì• Starting disk-based merging process...\n",
      "üîπ Writing C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2021.csv as the base dataset...\n",
      "‚ö†Ô∏è Temporary file C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Final_Merged_Data.csv.tmp could not be renamed. Please ensure it is not open in another program.\n",
      "‚úÖ Final dataset saved at: C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Final_Merged_Data.csv.tmp\n",
      "üîπ Merging C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2022.csv in chunks...\n",
      "üîπ Merging C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2023.csv in chunks...\n",
      "üîπ Merging C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Merged_2024.csv in chunks...\n",
      "‚úÖ Final dataset saved at: C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Final_Merged_Data.csv.tmp\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define base path\n",
    "base_path = r\"C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\"\n",
    "\n",
    "# List of yearly merged files\n",
    "year_files = [\n",
    "    os.path.join(base_path, \"Merged_2021.csv\"),\n",
    "    os.path.join(base_path, \"Merged_2022.csv\"),\n",
    "    os.path.join(base_path, \"Merged_2023.csv\"),\n",
    "    os.path.join(base_path, \"Merged_2024.csv\"),\n",
    "]\n",
    "\n",
    "# Output file path\n",
    "final_output = os.path.join(base_path, \"Final_Merged_Data.csv\")\n",
    "temp_output = final_output + \".tmp\"\n",
    "if os.path.exists(temp_output):\n",
    "    os.remove(temp_output)\n",
    "if os.path.exists(final_output):\n",
    "    try:\n",
    "        os.remove(final_output)\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è Could not remove {final_output}. It may be open in another program. Writing to temporary file instead.\")\n",
    "        final_output = temp_output\n",
    "\n",
    "# Open first file and write to output in chunks\n",
    "chunk_size = 500_000  # Adjust based on available RAM\n",
    "\n",
    "print(\"üì• Starting disk-based merging process...\")\n",
    "\n",
    "with open(final_output, \"w\", newline=\"\") as f_out:\n",
    "    # Process the first file\n",
    "    first_file = year_files[0]\n",
    "    print(f\"üîπ Writing {first_file} as the base dataset...\")\n",
    "    \n",
    "    for chunk in pd.read_csv(first_file, chunksize=chunk_size):\n",
    "        chunk.to_csv(f_out, index=False, mode=\"w\", header=True)\n",
    "\n",
    "# If we wrote to a temporary file due to permission issues, attempt to rename it back.\n",
    "if final_output.endswith(\".tmp\"):\n",
    "    try:\n",
    "        os.replace(temp_output, final_output[:-4])\n",
    "        final_output = final_output[:-4]\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è Temporary file {temp_output} could not be renamed. Please ensure it is not open in another program.\")\n",
    "\n",
    "print(f\"‚úÖ Final dataset saved at: {final_output}\")\n",
    "\n",
    "for file in year_files[1:]:\n",
    "    print(f\"üîπ Merging {file} in chunks...\")\n",
    "    with open(final_output, \"a\", newline=\"\") as f_out:\n",
    "        for chunk in pd.read_csv(file, chunksize=chunk_size):\n",
    "            chunk.to_csv(f_out, index=False, mode=\"a\", header=False)\n",
    "\n",
    "print(f\"‚úÖ Final dataset saved at: {final_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Timestamp  Control Threshold  Load Control  Online Load\n",
      "0  2021-01-01 00:00:00              626.2           0.0       514.37\n",
      "1  2021-01-01 00:30:00              572.0           0.0       505.76\n",
      "2  2021-01-01 01:00:00              572.0           0.0       504.80\n",
      "3  2021-01-01 01:30:00              572.0           0.0       499.74\n",
      "4  2021-01-01 02:00:00              572.0           0.0       496.19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Final_Merged_Data.csv.tmp\"\n",
    "\n",
    "df = pd.read_csv(file_path) \n",
    "print(df.head())  # Check structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 455236\n",
      "\n",
      "Columns with missing values:\n",
      "Timestamp            385112\n",
      "Control Threshold    368368\n",
      "Load Control         368368\n",
      "Online Load          368424\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the total number of rows\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(\"\\nColumns with missing values:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row count per column:\n",
      "Timestamp            70124\n",
      "Control Threshold    86868\n",
      "Load Control         86868\n",
      "Online Load          86812\n",
      "dtype: int64\n",
      "‚ö†Ô∏è WARNING: Some columns have missing values!\n"
     ]
    }
   ],
   "source": [
    "# Check if all columns have the same number of non-null values\n",
    "row_counts = df.notnull().sum()\n",
    "\n",
    "print(\"\\nRow count per column:\")\n",
    "print(row_counts)\n",
    "\n",
    "# Check if all values match the expected row count\n",
    "if len(set(row_counts)) == 1:\n",
    "    print(\"‚úÖ All columns have the same number of rows!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: Some columns have missing values!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Warning: 385114 duplicate timestamps found!\n"
     ]
    }
   ],
   "source": [
    "# Count duplicate timestamps\n",
    "duplicate_timestamps = df[\"Timestamp\"].duplicated().sum()\n",
    "\n",
    "if duplicate_timestamps > 0:\n",
    "    print(f\"‚ö†Ô∏è Warning: {duplicate_timestamps} duplicate timestamps found!\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate timestamps found. Data alignment looks good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully renamed temp file to: C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Final_Merged_Data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the temporary file as a proper CSV\n",
    "final_output = r\"C:\\Users\\Linds\\Repos\\East_River\\Data\\Raw\\Final_Merged_Data.csv\"\n",
    "\n",
    "# Rename the temp file if it exists\n",
    "temp_file = final_output + \".tmp\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(temp_file):\n",
    "    try:\n",
    "        shutil.move(temp_file, final_output)\n",
    "        print(f\"‚úÖ Successfully renamed temp file to: {final_output}\")\n",
    "    except PermissionError:\n",
    "        print(f\"‚ö†Ô∏è Could not rename {temp_file}. Ensure it's not open elsewhere.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Temporary file {temp_file} not found. Please check the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "load_forecasting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
