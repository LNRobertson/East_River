{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fce0b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Linds\\anaconda3\\envs\\load_forecasting_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Linds\\anaconda3\\envs\\load_forecasting_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cv_mae    cv_rmse     ho_mae    ho_rmse\n",
      "horizon                                            \n",
      "24       11.752477  16.830566   9.544760  13.032488\n",
      "48       11.938297  16.732316  10.007958  13.590096\n",
      "72       12.362068  17.479692  10.359988  13.970080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Linds\\anaconda3\\envs\\load_forecasting_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports & Config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "import joblib\n",
    "\n",
    "# use your pre‑processed v2 dataset (already contains y_plus_*h)\n",
    "DATA_PATH    = r\"C:\\Users\\Linds\\Repos\\East_River\\data\\training\\east_river_training-v2.h5\"\n",
    "HORIZONS     = [24, 48, 72]\n",
    "TS_CV        = TimeSeriesSplit(n_splits=3)\n",
    "MODEL_PARAMS = dict(tree_method='hist', random_state=0)\n",
    "\n",
    "# 2. Load preprocessed data\n",
    "def load_data(path):\n",
    "    # key='df' matches how you saved v2\n",
    "    return pd.read_hdf(path, key='df')\n",
    "\n",
    "# 3. Define features & targets (no shifting needed)\n",
    "def prepare(df, horizons):\n",
    "    # drop leakage & existing targets\n",
    "    drop_cols = [\n",
    "        'local_time',\n",
    "        'last_control_time',\n",
    "        'OnLine_Load_MW',\n",
    "        'Load_Control_MW',\n",
    "        'Control_Threshold_MW'\n",
    "    ] + [f'y_plus_{h}h' for h in horizons]\n",
    "    feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "    # drop location if present\n",
    "    X = df[feature_cols].drop(columns=['location'], errors='ignore')\n",
    "    # map horizons to their y vectors\n",
    "    y = {h: df[f'y_plus_{h}h'] for h in horizons}\n",
    "    return X, y\n",
    "\n",
    "# 4. Train/Eval routine\n",
    "def run_pipeline():\n",
    "    df = load_data(DATA_PATH)\n",
    "    X, y_map = prepare(df, HORIZONS)\n",
    "    split = int(0.8 * len(X))\n",
    "\n",
    "    results = []\n",
    "    for h in HORIZONS:\n",
    "        y = y_map[h]\n",
    "        X_tr, X_ho = X.iloc[:split], X.iloc[split:]\n",
    "        y_tr, y_ho = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "        # CV on train slice\n",
    "        cv = cross_validate(\n",
    "            XGBRegressor(**MODEL_PARAMS),\n",
    "            X_tr, y_tr,\n",
    "            cv=TS_CV,\n",
    "            scoring=['neg_mean_absolute_error','neg_root_mean_squared_error'],\n",
    "            n_jobs=1\n",
    "        )\n",
    "\n",
    "        # final fit & hold‑out eval\n",
    "        model = XGBRegressor(**MODEL_PARAMS)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_ho)\n",
    "\n",
    "        mae  = mean_absolute_error(y_ho, y_pred)\n",
    "        rmse = mean_squared_error(y_ho, y_pred, squared=False)\n",
    "\n",
    "        # persist\n",
    "        joblib.dump(model, f\"xgb_v2_{h}h.pkl\")\n",
    "\n",
    "        results.append({\n",
    "            'horizon':  h,\n",
    "            'cv_mae':  -cv['test_neg_mean_absolute_error'].mean(),\n",
    "            'cv_rmse': -cv['test_neg_root_mean_squared_error'].mean(),\n",
    "            'ho_mae':   mae,\n",
    "            'ho_rmse':  rmse\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).set_index('horizon')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_res = run_pipeline()\n",
    "    print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2540c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Linds\\anaconda3\\envs\\load_forecasting_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Linds\\anaconda3\\envs\\load_forecasting_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Linds\\anaconda3\\envs\\load_forecasting_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV_MAE</th>\n",
       "      <th>CV_RMSE</th>\n",
       "      <th>HO_MAE</th>\n",
       "      <th>HO_RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.752477</td>\n",
       "      <td>16.830566</td>\n",
       "      <td>9.544760</td>\n",
       "      <td>13.032488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11.938297</td>\n",
       "      <td>16.732316</td>\n",
       "      <td>10.007958</td>\n",
       "      <td>13.590096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>12.362068</td>\n",
       "      <td>17.479692</td>\n",
       "      <td>10.359988</td>\n",
       "      <td>13.970080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CV_MAE    CV_RMSE     HO_MAE    HO_RMSE\n",
       "h                                             \n",
       "24  11.752477  16.830566   9.544760  13.032488\n",
       "48  11.938297  16.732316  10.007958  13.590096\n",
       "72  12.362068  17.479692  10.359988  13.970080"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Imports & Config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "import joblib\n",
    "\n",
    "DATA_PATH = r'C:\\Users\\Linds\\Repos\\East_River\\data\\training\\east_river_processed_dataset-v4.h5'\n",
    "HORIZONS = [24, 48, 72]\n",
    "TS_CV = TimeSeriesSplit(n_splits=3)\n",
    "MODEL_PARAMS = dict(tree_method='hist', random_state=0)\n",
    "\n",
    "# 2. Load & preprocess\n",
    "def load_and_prep(path):\n",
    "    df = pd.read_hdf(path)\n",
    "    # date features & encode\n",
    "    df['dow_sin'] = np.sin(2*np.pi*df.day_of_week_num/7)\n",
    "    df['dow_cos'] = np.cos(2*np.pi*df.day_of_week_num/7)\n",
    "    df.sort_values('local_time', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # bool→int & categorical codes\n",
    "    bools = df.select_dtypes(include='bool').columns\n",
    "    df[bools] = df[bools].astype(int)\n",
    "    for c in df.select_dtypes(exclude=[np.number]).columns.difference(['location','local_time']):\n",
    "        df[c] = df[c].astype('category').cat.codes\n",
    "    return df\n",
    "\n",
    "df = load_and_prep(DATA_PATH)\n",
    "\n",
    "# 3. Create shifted targets & drop leakage\n",
    "for H in HORIZONS:\n",
    "    df[f'y_plus_{H}h'] = df.OnLine_Load_MW.shift(-H)\n",
    "df.dropna(subset=[f'y_plus_{H}h' for H in HORIZONS], inplace=True)\n",
    "leak = ['local_time','last_control_time','OnLine_Load_MW','Load_Control_MW','Control_Threshold_MW']\n",
    "leak += [f'y_plus_{H}h' for H in HORIZONS]\n",
    "FEATURES = [c for c in df.columns if c not in leak]\n",
    "\n",
    "# 4. Train/Eval routine\n",
    "results = []\n",
    "for H in HORIZONS:\n",
    "    X = df[FEATURES].drop(columns='location')\n",
    "    y = df[f'y_plus_{H}h']\n",
    "    # hold‑out split\n",
    "    split = int(0.8*len(X))\n",
    "    X_tr, X_ho = X.iloc[:split], X.iloc[split:]\n",
    "    y_tr, y_ho = y.iloc[:split], y.iloc[split:]\n",
    "    # CV\n",
    "    cv = cross_validate(XGBRegressor(**MODEL_PARAMS),\n",
    "                        X_tr, y_tr,\n",
    "                        cv=TS_CV,\n",
    "                        scoring=['neg_mean_absolute_error','neg_root_mean_squared_error'],\n",
    "                        n_jobs=1)\n",
    "    # retrain & hold‑out\n",
    "    m = XGBRegressor(**MODEL_PARAMS)\n",
    "    m.fit(X_tr, y_tr)\n",
    "    y_pred = m.predict(X_ho)\n",
    "    mae, rmse = mean_absolute_error(y_ho, y_pred), mean_squared_error(y_ho, y_pred, squared=False)\n",
    "    joblib.dump(m, f'xgb_final_{H}h.pkl')\n",
    "    results.append({\n",
    "      'h':H,\n",
    "      'CV_MAE': -cv['test_neg_mean_absolute_error'].mean(),\n",
    "      'CV_RMSE': -cv['test_neg_root_mean_squared_error'].mean(),\n",
    "      'HO_MAE': mae,\n",
    "      'HO_RMSE': rmse\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results).set_index('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00504bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HO_MAE and HO_RMSE are the hold‑out (HO) set error metrics you compute after you train your model on the training slice and predict on the held‑out slice:\n",
    "\n",
    "• HO_MAE – mean absolute error on the hold‑out data:  \n",
    "    mae = mean_absolute_error(y_ho, y_pred)  \n",
    "\n",
    "• HO_RMSE – root mean squared error on the hold‑out data:  \n",
    "    rmse = mean_squared_error(y_ho, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d396e",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "Here’s what’s going on:\n",
    "\n",
    "• CV (cross‑validation)  \n",
    "    – You take your training slice (first 80% of X/y), split it into e.g. 3 time‑series folds (`TS_CV`)  \n",
    "    – For each fold you train on two sub‑folds and score on the third  \n",
    "    – You then average those fold scores to get `cv_mae` and `cv_rmse`  \n",
    "\n",
    "• HO (hold‑out)  \n",
    "    – You train a fresh model on the entire training slice (first 80%)  \n",
    "    – You predict on the reserved hold‑out slice (last 20%) and compute MAE/RMSE there  \n",
    "    – That gives you `ho_mae` and `ho_rmse`  \n",
    "\n",
    "The .pkl files  \n",
    "    – Are the trained XGBRegressor objects (one per horizon) serialized via `joblib.dump`  \n",
    "    – You can later load them with `joblib.load(\"xgb_v2_24h.pkl\")` (for example) to  \n",
    "        • Inspect the model  \n",
    "        • Make new predictions on unseen data  \n",
    "        • Deploy without retraining from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce60c3",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Detailed evaluation & visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reload data & split\n",
    "df     = load_data(DATA_PATH)\n",
    "X, y_map = prepare(df, HORIZONS)\n",
    "split  = int(0.8 * len(X))\n",
    "X_ho   = X.iloc[split:]\n",
    "\n",
    "fig, axes = plt.subplots(len(HORIZONS), 3, figsize=(15, 5 * len(HORIZONS)))\n",
    "\n",
    "for i, h in enumerate(HORIZONS):\n",
    "    model   = joblib.load(f\"xgb_v2_{h}h.pkl\")\n",
    "    y_true  = y_map[h].iloc[split:]\n",
    "    y_pred  = model.predict(X_ho)\n",
    "    resid   = y_true - y_pred\n",
    "\n",
    "    # 1) Residual histogram\n",
    "    ax = axes[i,0]\n",
    "    ax.hist(resid, bins=50, edgecolor='k')\n",
    "    ax.set_title(f\"{h}h Residuals\")\n",
    "    ax.set_xlabel(\"Error (True – Pred)\")\n",
    "    \n",
    "    # 2) Actual vs Pred scatter\n",
    "    ax = axes[i,1]\n",
    "    ax.scatter(y_true, y_pred, alpha=0.3)\n",
    "    mn, mx = y_true.min(), y_true.max()\n",
    "    ax.plot([mn,mx],[mn,mx],'r--')\n",
    "    ax.set_title(f\"{h}h Actual vs Pred\")\n",
    "    ax.set_xlabel(\"True\")\n",
    "    ax.set_ylabel(\"Pred\")\n",
    "\n",
    "    # 3) Top‐10 feature importance\n",
    "    ax = axes[i,2]\n",
    "    fi = pd.Series(model.feature_importances_, index=X_ho.columns)\n",
    "    fi.nlargest(10).sort_values().plot.barh(ax=ax)\n",
    "    ax.set_title(f\"{h}h Top‐10 Features\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compare against existing Control_Threshold logic\n",
    "import joblib\n",
    "df_full = pd.read_hdf(DATA_PATH, key='df')  # raw v2 includes actual & threshold\n",
    "print(\"\\n=== Hold‑out: Forecast vs Threshold ===\")\n",
    "for h in HORIZONS:\n",
    "    # align actual & threshold for H‑hour ahead\n",
    "    actual = df_full['OnLine_Load_MW'].shift(-h).dropna().iloc[split:]\n",
    "    threshold = df_full['Control_Threshold_MW'].shift(-h).dropna().iloc[split:]\n",
    "    # load your saved model and predict on the same hold‑out X\n",
    "    model = joblib.load(f\"xgb_v2_{h}h.pkl\")\n",
    "    y_pred = model.predict(X.iloc[split:])\n",
    "    # compute MAEs\n",
    "    mae_f = mean_absolute_error(actual, y_pred)\n",
    "    mae_t = mean_absolute_error(actual, threshold)\n",
    "    imp  = (mae_t - mae_f) / mae_t * 100\n",
    "    print(f\"{h}h — MAE(threshold)={mae_t:.2f}, MAE(forecast)={mae_f:.2f}, Δ={imp:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "load_forecasting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
